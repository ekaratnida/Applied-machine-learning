{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYFo9Gdy-BSw"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/weaviate-features/multi-vector/multi-vector-colipali-rag.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7SB9B-G-BSy"
      },
      "source": [
        "# Multimodal RAG over PDFs using ColQwen2, Qwen2.5, and Weaviate\n",
        "\n",
        "This notebook demonstrates [Multimodal Retrieval-Augmented Generation (RAG)](https://weaviate.io/blog/multimodal-rag) over PDF documents.\n",
        "We will be performing retrieval against a collection of PDF documents by embedding both the individual pages of the documents and our queries into the same multi-vector space, reducing the problem to approximate nearest-neighbor search on ColBERT-style multi-vector embeddings under the MaxSim similarity measure.\n",
        "\n",
        "For this purpose, we will use\n",
        "\n",
        "- **A multimodal [late-interaction model](https://weaviate.io/blog/late-interaction-overview)**, like ColPali and ColQwen2, to generate\n",
        "embeddings. This tutorial uses the publicly available model\n",
        "[ColQwen2-v1.0](https://huggingface.co/vidore/colqwen2-v1.0) with a permissive Apache 2.0 license.\n",
        "- **A Weaviate [vector database](https://weaviate.io/blog/what-is-a-vector-database)**, which  has a [multi-vector feature](https://docs.weaviate.io/weaviate/tutorials/multi-vector-embeddings) to effectively index a collection of PDF documents and support textual queries against the contents of the documents, including both text and figures.\n",
        "- **A vision language model (VLM)**, specifically [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), to support multimodal Retrieval-Augmented Generation (RAG).\n",
        "\n",
        "Below, you can see the multimodal RAG system overview:\n",
        "\n",
        "<img src=\"https://github.com/weaviate/recipes/blob/main/weaviate-features/multi-vector/figures/multimodal-rag-diagram.png?raw=1\" width=\"700px\"/>\n",
        "\n",
        "First, the ingestion pipeline processes the PDF documents as images with the multimodal late-interaction model. The multi-vector embeddings are stored in a vector database.\n",
        "Then at query time, the text query is processed by the same multimodal late-interaction model to retrieve the relevant documents.\n",
        "The retrieved PDF files are then passed as visual context together with the original user query to the vision language model, which generates a response based on this information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgo1k--1-deC"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "To run this notebook, you will need a machine capable of running neural networks using 5-10 GB of memory.\n",
        "The demonstration uses two different vision language models that both require several gigabytes of memory.\n",
        "See the documentation for each individual model and the general PyTorch docs to figure out how to best run the models on your hardware.\n",
        "\n",
        "For example, you can run it on:\n",
        "\n",
        "- Google Colab (using the free-tier T4 GPU)\n",
        "- or locally (tested on an M2 Pro Mac).\n",
        "\n",
        "Furthermore, you will need an instance of Weaviate version >= `1.29.0`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7iyXUkS3Aw8"
      },
      "source": [
        "## Step 1: Install required libraries\n",
        "\n",
        "Let's begin by installing and importing the required libraries.\n",
        "\n",
        "Note that you'll need Python `3.13`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q8JvCewx984G"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install colpali_engine weaviate-client qwen_vl_utils\n",
        "%pip install -q -U \"colpali-engine[interpretability]>=0.3.2,<0.4.0\"\n",
        "%pip install -U datasets pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "161XCtm33ctL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers.utils.import_utils import is_flash_attn_2_available\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "\n",
        "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
        "#from colpali_engine.models import ColPali, ColPaliProcessor # uncomment if you prefer to use ColPali models instead of ColQwen2 models\n",
        "\n",
        "import weaviate\n",
        "from weaviate.classes.init import Auth\n",
        "import weaviate.classes.config as wc\n",
        "from weaviate.classes.config import Configure\n",
        "from weaviate.classes.query import MetadataQuery\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from colpali_engine.interpretability import (\n",
        "    get_similarity_maps_from_embeddings,\n",
        "    plot_all_similarity_maps,\n",
        "    plot_similarity_map,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1-rtXlKCO2b",
        "outputId": "bcc79fe2-28b6-42a9-b3ad-25c211d2b198"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install -U datasets pypdf"
      ],
      "metadata": {
        "id": "mNQWNfVMF_rx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPYNXv3PJOCJ"
      },
      "source": [
        "## Step 2: Load the PDF dataset\n",
        "\n",
        "Let's start with the data.\n",
        "We're going to first load a PDF document dataset of the [top-40 most\n",
        "cited AI papers on arXiv](https://arxiv.org/abs/2412.12121) from Hugging Face from the period 2023-01-01 to 2024-09-30."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pypdf import PdfReader\n",
        "from datasets import Dataset\n",
        "\n",
        "reader = PdfReader(\"/content/drive/MyDrive/DADS.pdf\")\n",
        "text_data = [{\"page\": i, \"text\": page.extract_text()} for i, page in enumerate(reader.pages)]\n",
        "\n",
        "# 3. Create Dataset object\n",
        "dataset = Dataset.from_list(text_data)\n",
        "\n",
        "print(dataset) #num_rows is a number of pages."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewROQC6mG1ab",
        "outputId": "17666b2d-d0a0-4adc-fbd9-416dd17af7cf"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['page', 'text'],\n",
            "    num_rows: 22\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByLc7qPVEgVy",
        "outputId": "bce48b8e-9243-4f4e-ccea-d61bc35bfc64"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['page', 'text'],\n",
              "    num_rows: 22\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6uElnXHJlI0"
      },
      "source": [
        "Let's take a look at a sample document page from the loaded PDF dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60QabGH4JikX",
        "outputId": "d2348c8b-3b0b-4223-b683-5aa9b39efc4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'page': 7,\n",
              " 'text': '8 \\n \\n \\uf0a3 แบบทางไกลผ่านสื่อแพร่ภาพและเสียงเป็นสื่อหลัก \\n \\uf0a3 แบบทางไกลทางอิเล็กทรอนิกส์เป็นสื่อหลัก (E-learning) \\n \\uf0a3 แบบทางไกลทางอินเตอร์เน็ต \\n \\n 2.8 การเทียบโอนหน่วยกิต รายวิชาและการลงทะเบียนเรียนเข้าสถาบันอุดมศึกษา \\n หลักเกณฑ์การเทียบโอนหน่วยกิต ให้เป็นไปตามข้อบังคับของสถาบันบัณฑิตพัฒนบริหารศาสตร์ว่าด้วย\\nการศึกษา และ/หรือประกาศของคณะสถิติประยุกต์ \\n3. หลักสูตรและอาจารย์ผู้สอน \\n 3.1  หลักสูตร \\n 3.1.1 จำนวนหน่วยกิต \\n ตลอดหลักสูตรไม่น้อยกว่า 36 หน่วยกิต \\n 3.1.2 โครงสร้างหลักสูตร \\n แผน ก2 ทำวิทยานิพนธ์ แผน ข ไม่ทำวิทยานิพนธ์ \\nหมวดวิชาเสริมพื้นฐาน ไม่นับหน่วยกิต ไม่นับหน่วยกิต \\nหมวดวิชาพื้นฐาน 6 หน่วยกิต 6 หน่วยกิต \\nหมวดวิชาหลัก 15 หน่วยกิต 15 หน่วยกิต \\nหมวดวิชาเลือก 3 หน่วยกิต 12 หน่วยกิต \\nวิชาการค้นคว้าอิสระ - 3 หน่วยกิต \\nสอบประมวลความรู้ สอบ สอบ \\nสอบปากเปล่า - สอบ \\nวิทยานิพนธ์ \\n(ผ่านการสอบป้องกันวิทยานิพนธ์) \\n12 หน่วยกิต - \\nรวมไม่น้อยกว่า 36 หน่วยกิต 36 หน่วยกิต \\n3.1.3 รายวิชา \\n (1) หมวดวิชาเสริมพื้นฐาน หมายถึงวิชาที่มุ่งปรับความรู้ในระดับต่ำกว่าขั้นบัณฑิตศึกษาของ\\nนักศึกษาเพื่อให้พร้อมที่จะศึกษาในชั้นปริญญาโท ประกอบด้วย \\nสพ 4000 \\nND 4000 \\nพื้นฐานสำหรับบัณฑิตศึกษา  \\nFoundation for Graduate Studies \\n3(2–2-5) \\nภส 4001 \\nLC 4001 \\nการพัฒนาทักษะการอ่านภาษาอังกฤษสำหรับบัณฑิตศึกษา \\nReading Skills Development in English for Graduate Studies \\n3(2–2-5) \\nภส 4002 \\nLC 4002 \\nการพัฒนาทักษะภาษาอังกฤษแบบบูรณาการ \\nIntegrated English Language Skills Development \\n3(2–2-5) \\nภส 4011 \\nLC 4011 \\nการซ่อมเสริมการ พัฒนาทักษะการอ่านภาษาอังกฤษสำหรับบัณฑิตศึกษา  \\nRemedial Reading Skills Development in English for Graduate Studies \\n3(2–2-5) \\nภส 4012 \\nLC 4012 \\nการซ่อมเสริมการพัฒนาทักษะภาษาอังกฤษแบบบูรณาการ  \\nRemedial Integrated English Language Skills Development \\n3(2–2-5) '}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "dataset[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ9xfSazJHtp"
      },
      "source": [
        "## Step 3: Load the ColVision (ColPali or ColQwen2) model\n",
        "\n",
        "The approach to generate embeddings for this tutorial is outlined in the paper [ColPali: Efficient Document Retrieval with Vision Language Models](https://arxiv.org/abs/2407.01449). The paper demonstrates that it is possible to simplify traditional approaches to preprocessing PDF documents for retrieval:\n",
        "\n",
        "Traditional PDF processing in RAG systems involves using OCR (Optical Character Recognition) and layout detection software, and separate processing of text, tables, figures, and charts. Additionally, after text extraction, text processing also requires a chunking step. Instead, the ColPali method feeds images (screenshots) of entire PDF pages to a Vision Language Model that produces a ColBERT-style multi-vector embedding.\n",
        "\n",
        "<img src=\"https://github.com/weaviate/recipes/blob/main/weaviate-features/multi-vector/figures/colipali_pipeline.jpeg?raw=1\" width=\"700px\"/>\n",
        "\n",
        "There are different ColVision models, such as ColPali or ColQwen2, available, which mainly differ in the used encoders (Contextualized Late Interaction over Qwen2 vs. PaliGemma-3B). You can read more about the differences between ColPali and ColQwen2 in our [overview of late-interaction models](https://weaviate.io/blog/late-interaction-overview).\n",
        "\n",
        "Let's load the [ColQwen2-v1.0](https://huggingface.co/vidore/colqwen2-v1.0) model for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_KJSNaTMJZgH"
      },
      "outputs": [],
      "source": [
        "# Get rid of process forking deadlock warnings.\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzR5mJ1GZe-",
        "outputId": "178b0858-f555-4ce0-f757-ffb52b0e2204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Using attention implementation: eager\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): # If GPU available\n",
        "    device = \"cuda:0\"\n",
        "elif torch.backends.mps.is_available(): # If Apple Silicon available\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "if is_flash_attn_2_available():\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "    attn_implementation = \"eager\"\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Using attention implementation: {attn_implementation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-jSoTVTKHns"
      },
      "source": [
        "This notebook uses the ColQwen2 model because it has a permissive Apache 2.0 license.\n",
        "Alternatively, you can also use [ColPali](https://huggingface.co/vidore/colpali-v1.2), which has a Gemma license, or check out other available [ColVision models](https://github.com/illuin-tech/colpali). For a detailed comparison, you can also refer to [ViDoRe: The Visual Document Retrieval Benchmark](https://huggingface.co/spaces/vidore/vidore-leaderboard)\n",
        "\n",
        "If you want to use ColPali instead of ColQwen2, you can comment out the above code cell and uncomment the code cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPBXXz46MBW0"
      },
      "source": [
        "Before we go further, let's familiarize ourselves with the ColQwen2 model. It can create multi-vector embeddings from both images and text queries. Below you can see examples of each.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYtykMt5JuU1"
      },
      "source": [
        "## Step 4: Connect to a Weaviate vector database instance\n",
        "\n",
        "Now, you will need to connect to a running Weaviate vector database cluster.\n",
        "\n",
        "You can choose one of the following options:\n",
        "\n",
        "1. **Option 1:** You can create a 14-day free sandbox on the managed service [Weaviate Cloud (WCD)](https://console.weaviate.cloud/)\n",
        "2. **Option 2:** [Embedded Weaviate](https://docs.weaviate.io/deploy/installation-guides/embedded)\n",
        "3. **Option 3:** [Local deployment](https://docs.weaviate.io/deploy/installation-guides/docker-installation)\n",
        "4. [Other options](https://docs.weaviate.io/deploy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H15lTeNmD3bK"
      },
      "source": [
        "For this tutorial, you will need the Weaviate `v1.29.0` or higher.\n",
        "Let's make sure we have the required version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qxB3jcGDD-RA",
        "outputId": "20d9b6ee-3476-496f-e92b-457176a5a54c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.35.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "client.get_meta()['version']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZmkYAnQJ_FL"
      },
      "source": [
        "## Step 5: Create a collection\n",
        "\n",
        "Next, we will create a collection that will hold the embeddings of the images of the PDF document pages.\n",
        "\n",
        "We will not define a built-in vectorizer but use the [Bring Your Own Vectors (BYOV) approach](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors), where we manually embed queries and PDF documents at ingestions and query stage.\n",
        "\n",
        "Additionally, if you are interested in using the [MUVERA encoding algorithm](https://weaviate.io/blog/muvera) for multi-vector embeddings, you can uncomment it in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "i_l3EJviknzC"
      },
      "outputs": [],
      "source": [
        "collection_name = \"PDFDocuments3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "BoypH0Tv-BS0"
      },
      "outputs": [],
      "source": [
        "# Delete the collection if it already exists\n",
        "# Note: in practice, you shouldn't rerun this cell, as it deletes your data\n",
        "# in \"PDFDocuments\", and then you need to re-import it again.\n",
        "#if client.collections.exists(collection_name):\n",
        "#  client.collections.delete(collection_name)\n",
        "\n",
        "# Create a collection\n",
        "collection = client.collections.create(\n",
        "    name=collection_name,\n",
        "    properties=[\n",
        "        wc.Property(name=\"page\", data_type=wc.DataType.INT)\n",
        "    ],\n",
        "    vector_config=[\n",
        "        Configure.MultiVectors.self_provided(\n",
        "            name=\"dads_nida\",\n",
        "            encoding=Configure.VectorIndex.MultiVector.Encoding.muvera(),\n",
        "            vector_index_config=Configure.VectorIndex.hnsw(\n",
        "                multi_vector=Configure.VectorIndex.MultiVector.multi_vector()\n",
        "            )\n",
        "    )]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbpVPUpMk4jJ"
      },
      "source": [
        "## Step 6: Uploading the vectors to Weaviate\n",
        "\n",
        "In this step, we're indexing the vectors into our Weaviate Collection in batches.\n",
        "\n",
        "For each batch, the images are processed and encoded using the ColPali model, turning them into multi-vector embeddings.\n",
        "These embeddings are then converted from tensors into lists of vectors, capturing key details from each image and creating a multi-vector representation for each document.\n",
        "This setup works well with Weaviate's multivector capabilities.\n",
        "\n",
        "After processing, the vectors and any metadata are uploaded to Weaviate, gradually building up the index.\n",
        "You can lower or increase the `batch_size` depending on your available GPU resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s234AW0u-BS0",
        "outputId": "74b01647-0d49-4491-afa6-f8d158d034b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 1/22 Page objects to Weaviate.\n",
            "Added 2/22 Page objects to Weaviate.\n",
            "Added 3/22 Page objects to Weaviate.\n",
            "Added 4/22 Page objects to Weaviate.\n",
            "Added 5/22 Page objects to Weaviate.\n",
            "Added 6/22 Page objects to Weaviate.\n",
            "Added 7/22 Page objects to Weaviate.\n",
            "Added 8/22 Page objects to Weaviate.\n",
            "Added 9/22 Page objects to Weaviate.\n",
            "Added 10/22 Page objects to Weaviate.\n",
            "Added 11/22 Page objects to Weaviate.\n",
            "Added 12/22 Page objects to Weaviate.\n",
            "Added 13/22 Page objects to Weaviate.\n",
            "Added 14/22 Page objects to Weaviate.\n",
            "Added 15/22 Page objects to Weaviate.\n",
            "Added 16/22 Page objects to Weaviate.\n",
            "Added 17/22 Page objects to Weaviate.\n",
            "Added 18/22 Page objects to Weaviate.\n",
            "Added 19/22 Page objects to Weaviate.\n",
            "Added 20/22 Page objects to Weaviate.\n",
            "Added 21/22 Page objects to Weaviate.\n",
            "Added 22/22 Page objects to Weaviate.\n"
          ]
        }
      ],
      "source": [
        "# 1. Remove the page_images dictionary as it's no longer needed for text\n",
        "# page_texts = {} # Optional: if you want to keep a local cache of text\n",
        "\n",
        "with collection.batch.dynamic() as batch:\n",
        "    for i in range(len(dataset)):\n",
        "        p = dataset[i]\n",
        "\n",
        "        # 2. Extract the text content from your dataset\n",
        "        page_content = p[\"text\"]  # Ensure your dataset has a 'text' column\n",
        "\n",
        "        batch.add_object(\n",
        "            properties={\n",
        "                \"page_id\": p[\"page\"],\n",
        "                \"content\": page_content, # 3. Add the text to properties\n",
        "            },\n",
        "            # 4. Use the text vectorization method\n",
        "            # Assuming your embedder has a 'multi_vectorize_text' method\n",
        "            vector={\n",
        "                \"dads_nida\": colvision_embedder.multi_vectorize_text(page_content).cpu().float().numpy().tolist()\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "        print(f\"Added {i+1}/{len(dataset)} Page objects to Weaviate.\")\n",
        "\n",
        "    batch.flush()\n",
        "\n",
        "# Clean up\n",
        "del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgQGaZemnvsA",
        "outputId": "15513d72-1194-454d-9ced-c234ee60383f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "len(collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nznNtelLAZ7t"
      },
      "source": [
        "## Step 7: Multimodal Retrieval Query\n",
        "\n",
        "As an example of what we are going to build, consider the following actual demo query and resulting PDF page from our collection (nearest neighbor):\n",
        "\n",
        "- Query: \"How does DeepSeek-V2 compare against the LLaMA family of LLMs?\"\n",
        "- Nearest neighbor:  \"DeepSeek-V2: A Strong Economical and Efficient Mixture-of-Experts Language Model\" (arXiv: 2405.04434), Page: 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "focgDIfayLDT"
      },
      "outputs": [],
      "source": [
        "query = \"เอกรัฐ\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPyYupBB6k-4"
      },
      "source": [
        "Note: To avoid `OutOfMemoryError` on freely available resources like Google Colab, we will only retrieve a single document. If you have resources with more memory available, you can set the `limit`parameter to a higher value, like e.g., `limit=3` to increase the number of retrieved PDF pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q25GcAdO-BS0"
      },
      "outputs": [],
      "source": [
        "print(f\"The most relevant documents for the query \\\"{query}\\\" by order of relevance:\\n\")\n",
        "#result_images = []\n",
        "for i, o in enumerate(response.objects):\n",
        "    p = o.properties\n",
        "    print(\n",
        "        f\"{i+1}) MaxSim: {-o.metadata.distance:.2f}, \"\n",
        "        + f\"Page: \\\"{p['page_id']}\\\" \"\n",
        "        + f\"Text: {p['content']}), \"\n",
        "    )\n",
        "    #result_images.append(page_images[p[\"page_id\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3naCOvSFyXdA"
      },
      "source": [
        "The retrieved page with the highest MaxSim score is indeed the page with the figure we mentioned earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJZUXzK674e2"
      },
      "source": [
        "Let's check the similarity plot for the token \"MA\" in \"LLaMA\". (Note that similarity maps are created for each token separately.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir2zXq3XHIDf"
      },
      "source": [
        "## References\n",
        "\n",
        "- Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C., Colombo, P. (2024). ColPali: Efficient Document Retrieval with Vision Language Models. arXiv. https://doi.org/10.48550/arXiv.2407.01449\n",
        "- [ColPali GitHub repository](https://github.com/illuin-tech/colpali)\n",
        "- [ColPali Cookbook](https://github.com/tonywu71/colpali-cookbooks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U weaviate-client openai"
      ],
      "metadata": {
        "id": "FuJvbLHVRlJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WCD_URL = userdata.get(\"WEAVIATE_URL\")\n",
        "WCD_AUTH_KEY = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "\n",
        "# Weaviate Cloud Deployment\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WCD_URL,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(WCD_AUTH_KEY),\n",
        ")\n",
        "\n",
        "print(client.is_ready())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7QFMMyhS4PH",
        "outputId": "7bd376a2-51ed-4792-dd87-5b341f199278"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from openai import OpenAI\n",
        "\n",
        "# 1. Setup OpenRouter Client\n",
        "openrouter_client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "iPw8-qDzTTBq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Connect to Weaviate (Local or Cloud)\n",
        "def get_rag_answer(question):\n",
        "    # STEP 1: Search Weaviate for relevant Thai text\n",
        "    collection = client.collections.get(\"PDFDocuments3\")\n",
        "\n",
        "    # We use a simple search to get context\n",
        "    response = collection.query.near_vector(\n",
        "        near_vector=colvision_embedder.multi_vectorize_text(question).cpu().float().numpy(),\n",
        "        target_vector=\"dads_nida\",\n",
        "        limit=2,\n",
        "        return_metadata=MetadataQuery(distance=True), # Needed to return MaxSim score\n",
        "    )\n",
        "    #response.objects\n",
        "\n",
        "    context = \" \".join([obj.properties['content'] for obj in response.objects])\n",
        "    print(\"context = \", context[:10])\n",
        "\n",
        "    # STEP 2: Send Context + Question to OpenRouter (Free Model)\n",
        "    response = openrouter_client.chat.completions.create(\n",
        "        #model=\"qwen/qwen-2.5-72b-instruct:free\",\n",
        "        #model=\"qwen/qwen-2.5-vl-7b-instruct:free\",\n",
        "        model = \"meta-llama/llama-3.3-70b-instruct:free\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"โปรดตอบคำถามโดยใช้ข้อมูลจากบริบทที่ให้มาเท่านั้น (Answer in Thai using the provided context).\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Ossq8pSmTWP1"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "answer = get_rag_answer(\"สรุปภาพรวมของหลักสูตร DADS\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPuZD2e4R3KY",
        "outputId": "98d838d5-6fb9-47e6-de04-c0e569748797"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context =  10 \n",
            " \n",
            "วขวข\n",
            "หลักสูตร DADS ประกอบด้วย 7 หมวดวิชา ได้แก่\n",
            "\n",
            "1. หมวดวิชาเสริมพื้นฐาน (9 หน่วยกิต)\n",
            "2. หมวดวิชาพื้นฐาน (6 หน่วยกิต)\n",
            "3. หมวดวิชาหลัก (15 หน่วยกิต)\n",
            "4. หมวดวิชาเลือก (อย่างน้อย 3-12 หน่วยกิต)\n",
            "5. หมวดวิชาสัมมนาและการศึกษาเฉพาะเรื่อง (3-6 หน่วยกิต)\n",
            "6. หมวดวิชาการค้นคว้าอิสระ (3 หน่วยกิต)\n",
            "7. หมวดวิทยานิพนธ์ (12 หน่วยกิต)\n",
            "\n",
            "โดยหลักสูตรนี้มุ่งให้นักศึกษามีความรู้ความชำนาญเฉพาะด้านการวิเคราะห์ข้อมูลและวิทยาการข้อมูล และสามารถเลือกเรียนวิชาเสรีจากหลักสูตรอื่น ๆ ได้ตามความเหมาะสม\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}