{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3710jvsc74a57bd0563d867f513b1db22a50c0772d3498166496ad16228d7c1d495fd4c7e7a93ad6",
      "display_name": "Python 3.7.10 64-bit ('graph': conda)"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "name": "20_modeling_pytoch.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4714VKOKo8cK"
      },
      "source": [
        "# Modeling\n",
        "\n",
        "This notebook will guide you through the process of loading heterogeneous graphs and training models. The heterogeneous graph is based on Deep Graph Library (DGL) implementation, and the training process is based on PyTorch implementation.\n",
        "\n",
        "The CPU is recommended for training when the equipment allows it. If you need to use GPU, please install GPU-based DGL separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsBPiu9io8cR"
      },
      "source": [
        "## Colab setting\n",
        "\n",
        "If you want to train in Colab, please run both cells first and mount to the corresponding path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AICBHAJHa56",
        "outputId": "1b262b49-1143-4d78-bbda-549cb886b90e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtJfGQ6GHh88",
        "outputId": "6a033273-08dd-46da-95a8-7e0b12930e7f"
      },
      "source": [
        "import os\n",
        "cur_path = \"/content/drive/MyDrive/graph-fraud-detection/\"\n",
        "os.chdir(cur_path)\n",
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/graph-fraud-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aprSAaVaHiEX",
        "outputId": "253ee7f7-8aee-4985-a02d-975e25d85c07"
      },
      "source": [
        "#!pip install dgl # This might install a version compatible with the current PyTorch\n",
        "#!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/repo.html\n",
        "!pip install dgl==1.1.2  # Or other version that works without GraphBolt\n",
        "\n",
        "#-cu101"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dgl==1.1.2 in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (1.15.3)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.2) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torchdata==0.9.0"
      ],
      "metadata": {
        "id": "aHMqrkt8j6wV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgVPj47IMMG4"
      },
      "source": [
        "## Training (All in 1)\n",
        "\n",
        "In this part, you can use this all-in-one method to train the model easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0mfuNcvBy6n"
      },
      "source": [
        "#!python train.py --n-epochs 1000"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSmqRg1eo8cT"
      },
      "source": [
        "## Training (Detailed)\n",
        "\n",
        "Besides the approach mentioned before, you can also use this detailed approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZWNPZTPOJSb"
      },
      "source": [
        "### Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "E6TREv-bfXKZ",
        "outputId": "4e22c944-4775-4e82-d22c-9e0d85e5b8b0"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "\n",
        "os.environ['DGLBACKEND'] = 'pytorch'\n",
        "\n",
        "import torch as th\n",
        "import dgl\n",
        "import numpy as np\n",
        "\n",
        "from gnn.estimator_fns import *\n",
        "from gnn.graph_utils import *\n",
        "from gnn.data import *\n",
        "from gnn.utils import *\n",
        "from gnn.pytorch_model import *\n",
        "from train import *"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gnn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3707331241.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_fns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gnn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9CmUjgaHXdj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E881gvwobfSz"
      },
      "source": [
        "### Load data\n",
        "\n",
        "Recall the edges we defined before and the csv files we used to save them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmqGrWm-HXdk"
      },
      "source": [
        "file_list = glob.glob('./data/*edgelist.csv')\n",
        "\n",
        "edges = \",\".join(map(lambda x: x.split(\"/\")[-1], [file for file in file_list if \"relation\" in file]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE2--V7AbfS0"
      },
      "source": [
        "To train the graph neural network, we need to define a few hyperparameters that determine properties such as the class of graph neural network models we will be using, the network architecture and the optimizer and optimization parameters.\n",
        "\n",
        "Here we're setting only a few of the hyperparameters, to see all the hyperparameters and their default values, see `gnn/estimator_fns.py`. The parameters set below are:\n",
        "\n",
        "- **nodes** is the name of the file that contains the node_ids of the target nodes and the node features.\n",
        "- **edges** is a regular expression that when expanded lists all the filenames for the edgelists\n",
        "- **labels** is the name of the file tha contains the target node_ids and their labels\n",
        "- **model** specify which graph neural network to use, this should be set to r-gcn\n",
        "\n",
        "The following hyperparameters can be tuned and adjusted to improve model performance\n",
        "\n",
        "- **batch-size** is the number nodes that are used to compute a single forward pass of the GNN\n",
        "- **embedding-size** is the size of the embedding dimension for non target nodes\n",
        "- **n-neighbors** is the number of neighbours to sample for each target node during graph sampling for mini-batch training\n",
        "- **n-layers** is the number of GNN layers in the model\n",
        "- **n-epochs** is the number of training epochs for the model training job\n",
        "- **optimizer** is the optimization algorithm used for gradient based parameter updates\n",
        "- **lr** is the learning rate for parameter updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDQKAwpYo8cW"
      },
      "source": [
        "### Generate graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23aFsML_bfS1"
      },
      "source": [
        "print('numpy version:{} PyTorch version:{} DGL version:{}'.format(np.__version__,\n",
        "                                                                    th.__version__,\n",
        "                                                                    dgl.__version__))\n",
        "\n",
        "args = parse_args()\n",
        "print(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4PfzLQKbfS1"
      },
      "source": [
        "args.edges = edges\n",
        "\n",
        "args.edges = get_edgelists('relation*', args.training_dir)\n",
        "\n",
        "g, features, target_id_to_node, id_to_node = construct_graph(args.training_dir,\n",
        "                                                                args.edges,\n",
        "                                                                args.nodes,\n",
        "                                                                args.target_ntype)\n",
        "\n",
        "mean, stdev, features = normalize(th.from_numpy(features))\n",
        "\n",
        "print('feature mean shape:{}, std shape:{}'.format(mean.shape, stdev.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bw1ceNebfS2"
      },
      "source": [
        "g.nodes['target'].data['features'] = features\n",
        "\n",
        "print(\"Getting labels\")\n",
        "n_nodes = g.number_of_nodes('target')\n",
        "\n",
        "labels, _, test_mask = get_labels(target_id_to_node,\n",
        "                                            n_nodes,\n",
        "                                            args.target_ntype,\n",
        "                                            os.path.join(args.training_dir, args.labels),\n",
        "                                            os.path.join(args.training_dir, args.new_accounts))\n",
        "print(\"Got labels\")\n",
        "\n",
        "labels = th.from_numpy(labels).float()\n",
        "test_mask = th.from_numpy(test_mask).float()\n",
        "\n",
        "n_nodes = th.sum(th.tensor([g.number_of_nodes(n_type) for n_type in g.ntypes]))\n",
        "n_edges = th.sum(th.tensor([g.number_of_edges(e_type) for e_type in g.etypes]))\n",
        "\n",
        "print(\"\"\"----Data statistics------'\n",
        "            #Nodes: {}\n",
        "            #Edges: {}\n",
        "            #Features Shape: {}\n",
        "            #Labeled Test samples: {}\"\"\".format(n_nodes,\n",
        "                                                    n_edges,\n",
        "                                                    features.shape,\n",
        "                                                    test_mask.sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjhjypJEo8cX"
      },
      "source": [
        "### Start training\n",
        "\n",
        "The training process and result will be saved in the same folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UETWzSN6bfS3"
      },
      "source": [
        "if args.num_gpus:\n",
        "    cuda = True\n",
        "    device = th.device('cuda:0')\n",
        "else:\n",
        "    cuda = False\n",
        "    device = th.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWQp2MxLHXdo"
      },
      "source": [
        "print(\"Initializing Model\")\n",
        "in_feats = features.shape[1]\n",
        "n_classes = 2\n",
        "\n",
        "ntype_dict = {n_type: g.number_of_nodes(n_type) for n_type in g.ntypes}\n",
        "\n",
        "model = get_model(ntype_dict, g.etypes, vars(args), in_feats, n_classes, device)\n",
        "print(\"Initialized Model\")\n",
        "\n",
        "features = features.to(device)\n",
        "\n",
        "labels = labels.long().to(device)\n",
        "test_mask = test_mask.to(device)\n",
        "# g = g.to(device)\n",
        "\n",
        "loss = th.nn.CrossEntropyLoss()\n",
        "\n",
        "# print(model)\n",
        "optim = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "print(\"Starting Model training\")\n",
        "\n",
        "initial_record()\n",
        "\n",
        "model, class_preds, pred_proba = train_fg(model, optim, loss, features, labels, g, g,\n",
        "                                            test_mask, device, args.n_epochs,\n",
        "                                            args.threshold,  args.compute_metrics)\n",
        "print(\"Finished Model training\")\n",
        "\n",
        "print(\"Saving model\")\n",
        "\n",
        "if not os.path.exists(args.model_dir):\n",
        "    os.makedirs(args.model_dir)\n",
        "\n",
        "save_model(g, model, args.model_dir, id_to_node, mean, stdev)\n",
        "print(\"Model and metadata saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u3-n_xdoZLc"
      },
      "source": [
        "%tb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2be32852"
      },
      "source": [
        "!pip install torchdata"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}