{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaratnida/Applied-machine-learning/blob/master/mistral/ocr/structured_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FPiAIwHteCl"
      },
      "source": [
        "# OCR Cookbook\n",
        "\n",
        "---\n",
        "\n",
        "## OCR Exploration and Simple Structured Outputs (Deprecated)\n",
        "In this cookbook, we will explore the basics of OCR and leverage it together with existing models to achieve structured outputs fueled by our OCR model (we recommend using the new Annotations feature instead for better results).\n",
        "\n",
        "You may want to do this in case current vision models are not powerful enough, hence enhancing their vision OCR capabilities with the OCR model to achieve better structured data extraction.\n",
        "\n",
        "---\n",
        "\n",
        "### Model Used\n",
        "- Mistral OCR\n",
        "- Pixtral 12B & Ministral 8B\n",
        "\n",
        "---\n",
        "\n",
        "**For a more up to date guide on structured outputs visit our [Annotations cookbook](https://github.com/mistralai/cookbook/blob/main/mistral/ocr/data_extraction.ipynb) on Data Extraction.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgZW4ZfetwAl"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install `mistralai` and download the required files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "po7Cukllt8za"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download PDF and image files"
      ],
      "metadata": {
        "id": "g8rxv4Tx5kNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MtKgrASwF3Ol"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/mistral7b.pdf\n",
        "!wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhwM0aITt7ti"
      },
      "source": [
        "## Mistral OCR with PDF\n",
        "\n",
        "We will need to set up our client. You can create an API key on our [Plateforme](https://console.mistral.ai/api-keys/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "odfkuCk6qSAw"
      },
      "outputs": [],
      "source": [
        "# Initialize Mistral client with API key\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = \"API_KEY\" # Replace with your API key\n",
        "client = Mistral(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-3YwljuFKK"
      },
      "source": [
        "There are two types of files you can apply OCR to:\n",
        "- 1. PDF files\n",
        "- 2. Image files\n",
        "\n",
        "Let's start with a PDF file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "svaJGBFlqm7_",
        "outputId": "36e7f400-d56e-4753-d8a1-d4e8ceaabf74"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-db8caf07-af3e-441a-9df0-b1ef6d5555fa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-db8caf07-af3e-441a-9df0-b1ef6d5555fa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Week02_single_regression_1_4_2024 (2).pdf to Week02_single_regression_1_4_2024 (2) (4).pdf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SDKError",
          "evalue": "API error occurred: Status 401\n{\"detail\":\"Unauthorized\"}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-4268292402.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Upload PDF file to Mistral's OCR service\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m uploaded_file = client.files.upload(\n\u001b[0m\u001b[1;32m     14\u001b[0m     file={\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpdf_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mistralai/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(self, file, purpose, retries, server_url, timeout_ms, http_headers)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4XX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mhttp_res_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             raise models.SDKError(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;34m\"API error occurred\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_res_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             )\n",
            "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 401\n{\"detail\":\"Unauthorized\"}"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from pathlib import Path\n",
        "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "import json\n",
        "\n",
        "# Verify PDF file exists\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "pdf_file = Path(list(uploaded.keys())[0])\n",
        "assert pdf_file.is_file()\n",
        "\n",
        "# Upload PDF file to Mistral's OCR service\n",
        "uploaded_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": pdf_file.stem,\n",
        "        \"content\": pdf_file.read_bytes(),\n",
        "    },\n",
        "    purpose=\"ocr\",\n",
        ")\n",
        "\n",
        "# Get URL for the uploaded file\n",
        "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "# Process PDF with OCR, including embedded images\n",
        "pdf_response = client.ocr.process(\n",
        "    document=DocumentURLChunk(document_url=signed_url.url),\n",
        "    model=\"mistral-ocr-latest\",\n",
        "    include_image_base64=True\n",
        ")\n",
        "\n",
        "# Convert response to JSON format\n",
        "response_dict = json.loads(pdf_response.model_dump_json())\n",
        "\n",
        "print(json.dumps(response_dict, indent=4)[0:1000]) # check the first 1000 characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG2_TdlKIxYs"
      },
      "source": [
        "View the result with the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxefUpm-Idp8"
      },
      "outputs": [],
      "source": [
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
        "    \"\"\"\n",
        "    Replace image placeholders in markdown with base64-encoded images.\n",
        "\n",
        "    Args:\n",
        "        markdown_str: Markdown text containing image placeholders\n",
        "        images_dict: Dictionary mapping image IDs to base64 strings\n",
        "\n",
        "    Returns:\n",
        "        Markdown text with images replaced by base64 data\n",
        "    \"\"\"\n",
        "    for img_name, base64_str in images_dict.items():\n",
        "        markdown_str = markdown_str.replace(\n",
        "            f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\"\n",
        "        )\n",
        "    return markdown_str\n",
        "\n",
        "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
        "    \"\"\"\n",
        "    Combine OCR text and images into a single markdown document.\n",
        "\n",
        "    Args:\n",
        "        ocr_response: Response from OCR processing containing text and images\n",
        "\n",
        "    Returns:\n",
        "        Combined markdown string with embedded images\n",
        "    \"\"\"\n",
        "    markdowns: list[str] = []\n",
        "    # Extract images from page\n",
        "    for page in ocr_response.pages:\n",
        "        image_data = {}\n",
        "        for img in page.images:\n",
        "            image_data[img.id] = img.image_base64\n",
        "        # Replace image placeholders with actual images\n",
        "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
        "\n",
        "    return \"\\n\\n\".join(markdowns)\n",
        "\n",
        "# Display combined markdowns and images\n",
        "display(Markdown(get_combined_markdown(pdf_response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yk5tBpPuKal"
      },
      "source": [
        "## Mistral OCR with Image\n",
        "\n",
        "In addition to the PDF file shown above, Mistral OCR can also process image files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFdyKIcgrahm"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "# Verify image exists\n",
        "image_file = Path(\"receipt.png\")\n",
        "assert image_file.is_file()\n",
        "\n",
        "# Encode image as base64 for API\n",
        "encoded = base64.b64encode(image_file.read_bytes()).decode()\n",
        "base64_data_url = f\"data:image/jpeg;base64,{encoded}\"\n",
        "\n",
        "# Process image with OCR\n",
        "image_response = client.ocr.process(\n",
        "    document=ImageURLChunk(image_url=base64_data_url),\n",
        "    model=\"mistral-ocr-latest\"\n",
        ")\n",
        "\n",
        "# Convert response to JSON\n",
        "response_dict = json.loads(image_response.model_dump_json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWStbt7LuMvT"
      },
      "source": [
        "## Extract structured data from OCR results\n",
        "\n",
        "OCR results can be further processed using another model.\n",
        "\n",
        "Our goal is to extract structured data from these results. To achieve this, we will utilize the `pixtral-12b-latest` model, supported by our OCR model, to deliver better and higher-quality answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aZOQs38r0GO"
      },
      "outputs": [],
      "source": [
        "# Get OCR results for processing\n",
        "image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "# Get structured response from model\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"pixtral-12b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                ImageURLChunk(image_url=base64_data_url),\n",
        "                TextChunk(\n",
        "                    text=(\n",
        "                        f\"This is image's OCR in markdown:\\n\\n{image_ocr_markdown}\\n.\\n\"\n",
        "                        \"Convert this into a sensible structured json response. \"\n",
        "                        \"The output should be strictly be json with no extra commentary\"\n",
        "                    )\n",
        "                ),\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# Parse and return JSON response\n",
        "response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "print(json.dumps(response_dict, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YKioib1vgTZ"
      },
      "source": [
        "In the example above, we are leveraging a model already capable of vision tasks.\n",
        "\n",
        "However, we could also use text-only models for the structured output. Note in this case, we do not include the image in the user message:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get OCR results for processing\n",
        "image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "# Get structured response from model\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"ministral-8b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                TextChunk(\n",
        "                    text=(\n",
        "                        f\"This is image's OCR in markdown:\\n\\n{image_ocr_markdown}\\n.\\n\"\n",
        "                        \"Convert this into a sensible structured json response. \"\n",
        "                        \"The output should be strictly be json with no extra commentary\"\n",
        "                    )\n",
        "                ),\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# Parse and return JSON response\n",
        "response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "print(json.dumps(response_dict, indent=4))\n",
        "\n",
        "import pandas as pd\n",
        "a_json = json.loads(json.dumps(response_dict,indent=4))\n",
        "print(a_json)\n",
        "df = pd.DataFrame.from_dict([a_json]) # Wrap a_json in a list\n",
        "df.to_excel(\"data.xlsx\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m19STu2DDfI",
        "outputId": "0d71e66a-aff5-45f6-ff41-d7098ab84568"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"city\": \"Palo Alto\",\n",
            "    \"expiration_date_time\": \"2024-08-19T23:59:00Z\",\n",
            "    \"purchase_date_time\": \"2024-08-19T13:34:00Z\",\n",
            "    \"total_due\": 15.0,\n",
            "    \"total_paid\": 15.0,\n",
            "    \"ticket_number\": \"00005883\",\n",
            "    \"serial_number\": \"520117260957\",\n",
            "    \"setting\": \"Permit Machines\",\n",
            "    \"machine_name\": \"Civic Center\",\n",
            "    \"payment_method\": \"Visa\",\n",
            "    \"display_instruction\": \"DISPLAY FACE UP ON DASH\"\n",
            "}\n",
            "{'city': 'Palo Alto', 'expiration_date_time': '2024-08-19T23:59:00Z', 'purchase_date_time': '2024-08-19T13:34:00Z', 'total_due': 15.0, 'total_paid': 15.0, 'ticket_number': '00005883', 'serial_number': '520117260957', 'setting': 'Permit Machines', 'machine_name': 'Civic Center', 'payment_method': 'Visa', 'display_instruction': 'DISPLAY FACE UP ON DASH'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc__PKmkwUnZ"
      },
      "source": [
        "## All Together - Mistral OCR + Custom Structured Output\n",
        "Let's design a simple function that takes an `image_path` file and returns a JSON structured output in a specific format. In this case, we arbitrarily decided we wanted an output respecting the following:\n",
        "\n",
        "```python\n",
        "class StructuredOCR:\n",
        "    file_name: str  # can be any string\n",
        "    topics: list[str]  # must be a list of strings\n",
        "    languages: str  # string\n",
        "    ocr_contents: dict  # any dictionary, can be freely defined by the model\n",
        "```\n",
        "\n",
        "We will make use of [custom structured outputs](https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM2ensmIwh4H"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "from pydantic import BaseModel\n",
        "import base64\n",
        "\n",
        "\n",
        "class StructuredOCR(BaseModel):\n",
        "    file_name: str\n",
        "    topics: list[str]\n",
        "    languages: str\n",
        "    ocr_contents: dict\n",
        "\n",
        "def structured_ocr(image_path: str) -> StructuredOCR:\n",
        "    \"\"\"\n",
        "    Process an image using OCR and extract structured data.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the image file to process\n",
        "\n",
        "    Returns:\n",
        "        StructuredOCR object containing the extracted data\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the image file does not exist\n",
        "    \"\"\"\n",
        "    # Validate input file\n",
        "    image_file = Path(image_path)\n",
        "    assert image_file.is_file(), \"The provided image path does not exist.\"\n",
        "\n",
        "    # Read and encode the image file\n",
        "    encoded_image = base64.b64encode(image_file.read_bytes()).decode()\n",
        "    base64_data_url = f\"data:image/jpeg;base64,{encoded_image}\"\n",
        "\n",
        "    # Process the image using OCR\n",
        "    image_response = client.ocr.process(\n",
        "        document=ImageURLChunk(image_url=base64_data_url),\n",
        "        model=\"mistral-ocr-latest\"\n",
        "    )\n",
        "    image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "    # Parse the OCR result into a structured JSON response\n",
        "    chat_response = client.chat.parse(\n",
        "        model=\"pixtral-12b-latest\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    ImageURLChunk(image_url=base64_data_url),\n",
        "                    TextChunk(text=(\n",
        "                        f\"This is the image's OCR in markdown:\\n{image_ocr_markdown}\\n.\\n\"\n",
        "                        \"Convert this into a structured JSON response \"\n",
        "                        \"with the OCR contents in a sensible dictionnary.\"\n",
        "                        )\n",
        "                    )\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        response_format=StructuredOCR,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    return chat_response.choices[0].message.parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVipACEOAyEX"
      },
      "source": [
        "We can now extract structured output from any image parsed with our OCR model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvt3OAcpyXCF"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "image_path = \"receipt.png\" # Path to sample receipt image\n",
        "structured_response = structured_ocr(image_path) # Process image and extract data\n",
        "\n",
        "# Parse and return JSON response\n",
        "response_dict = json.loads(structured_response.model_dump_json())\n",
        "print(json.dumps(response_dict, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8705WaqA8KV"
      },
      "source": [
        "The original image for comparison can be found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xj9tOTKA7mw"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image.resize((image.width // 5, image.height // 5))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yC4udP5Bpe72"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}